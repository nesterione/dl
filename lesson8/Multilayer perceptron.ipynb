{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = np.loadtxt('train.csv', delimiter=',', skiprows=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split on training pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features shape: (42000, 784)\n",
      "Labels shape: (42000,)\n"
     ]
    }
   ],
   "source": [
    "labels = train_data[:,0].astype(np.int32)  # labels\n",
    "X_train = train_data[:,1:]                 # features\n",
    "print('Features shape:', X_train.shape)\n",
    "print('Labels shape:', labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max intensity value of pixel before normalization: 255.0\n",
      "Max intensity value of pixel after normalization: 1.0\n"
     ]
    }
   ],
   "source": [
    "max_intensity = X_train.max()\n",
    "print('Max intensity value of pixel before normalization:', max_intensity)\n",
    "X_train = X_train / max_intensity # 255 - is a max intensity value of pixel\n",
    "print('Max intensity value of pixel after normalization:', X_train.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To categorical / labels one-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique labels: [0 1 2 3 4 5 6 7 8 9]\n",
      "Number of classes: 10\n"
     ]
    }
   ],
   "source": [
    "unique_labels = np.unique(labels)\n",
    "print('Unique labels:', unique_labels)\n",
    "N_CLASSES = unique_labels.size\n",
    "print('Number of classes:', N_CLASSES) # 10 classes for 10 digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels before encoding:\n",
      "[1 4 0]\n",
      "\n",
      "Encoded labels:\n",
      "[[0 1 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 1 0 0 0 0 0]\n",
      " [1 0 0 0 0 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "print('Labels before encoding:\\n{}\\n'.format(labels[2:5]))\n",
    "y_train = np.eye(N_CLASSES, dtype='int32')[labels]\n",
    "print('Encoded labels:\\n{}'.format(y_train[2:5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# sigmoid activation\n",
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "# sigmoid derivative\n",
    "def sigmoid_derivative(x):\n",
    "    # g(x)*(1-g(x)) \n",
    "    return sigmoid(x)*(1-sigmoid(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should to verify our functions. To do it we can plot graphics of sigmoid and it's derivative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAe0AAACcCAYAAABSvfMrAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x16cb8cf8898>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# test sigmoid and it's derivative\n",
    "test_arr = np.linspace(-10, 10) # Return evenly spaced numbers over a specified interval\n",
    "# create variable that stores figure\n",
    "plt.figure(figsize=(8, 2)) \n",
    "# first plot\n",
    "ax1 = plt.subplot(121)\n",
    "ax1.set_title('Sigmoid')\n",
    "ax1.plot(test_arr, sigmoid(test_arr))\n",
    "# second plot\n",
    "ax2 = plt.subplot(122)\n",
    "ax2.set_title('Sigmoid derivative')\n",
    "ax2.plot(test_arr, sigmoid_derivative(test_arr))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def categorical_crossentropy(y_pred, y):\n",
    "    \"\"\" y_pred - logits\n",
    "        y - labels\n",
    "    \"\"\"\n",
    "    # positive part of logical loss\n",
    "    # y > 0\n",
    "    pos = -y*np.log(y_pred)\n",
    "    # negative part of logical loss\n",
    "    # y < 0\n",
    "    neg = -(1-y)*np.log(1-y_pred)\n",
    "    # we sum binary cross entropy loss over every cell in one-hot vector\n",
    "    # E.g. for 5 classes - 5 error values, that should be summarized\n",
    "    cost = np.sum(np.nan_to_num(pos+neg), axis=1)\n",
    "    # we should return mean value over every training sample\n",
    "    return np.mean(cost)\n",
    "\n",
    "def categorical_crossentropy_derivative(y_pred, y):\n",
    "    \"\"\" y_pred - logits\n",
    "        y - labels\n",
    "    \"\"\"\n",
    "    return y_pred-y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we should to verify our cost function. To do it we choose 5 label's values from our training set and create an array of length 10, where 5 values is correct and another 5 - zeros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.6755577851529093"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test entropy\n",
    "# array of length ten\n",
    "test_arr_x = np.zeros(10, dtype=np.int32)\n",
    "# choose 5 correct labels\n",
    "test_arr_x[5:] = labels[5:10].copy()\n",
    "# to categorical\n",
    "test_arr_x = np.eye(N_CLASSES, dtype='int32')[test_arr_x]\n",
    "# we add some real values to avoid NaN\n",
    "test_arr_x = test_arr_x*0.99+0.00001\n",
    "# to evaluate we choose our true labels\n",
    "test_arr_z = y_train[:10].copy()\n",
    "categorical_crossentropy(test_arr_x[:5], test_arr_z[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weight initializer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def init_weights(sizes):\n",
    "    # sizes is a number of neurons per layer\n",
    "    # first layer is input and dont need any weights\n",
    "    # np.random.randn - random normal distribution\n",
    "    b = [np.random.randn(1,y) for y in sizes[1:]]\n",
    "    # weights\n",
    "    # sizes[:-1] - every except last\n",
    "    # sizes[1:] - every except first\n",
    "    # zip creates generator\n",
    "    W = [np.random.randn(y, x)/np.sqrt(x) \n",
    "            for x, y in zip(sizes[1:], sizes[:-1])]\n",
    "    \n",
    "    return (W, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To verify our initializator we can print weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight shapes: [(784, 10)]\n",
      "Bias shapes: [(1, 10)]\n",
      "Bias:\n",
      "[[ 0.40627422 -0.79427131 -2.4987268   1.1948303   1.42406284 -0.66233307\n",
      "  -1.79327779 -0.06663006 -0.66061186  0.91397532]]\n"
     ]
    }
   ],
   "source": [
    "W,b = init_weights([784,10])\n",
    "\n",
    "print('Weight shapes:', [w.shape for w in W])\n",
    "print('Bias shapes:', [b_.shape for b_ in b])\n",
    "print('Bias:')\n",
    "print(b[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def feedprop(x, W, b):\n",
    "    \"\"\"Return the output of \n",
    "    the network if ``x`` is input.\n",
    "    W - weights\n",
    "    b - biases\n",
    "    \n",
    "    Return a, z, act\n",
    "        a - array, predicted value\n",
    "        zs - list, variable that stores \n",
    "            all neuron info for every layer\n",
    "        acts - list, all activations for every layer\n",
    "              including input\n",
    "    \"\"\"\n",
    "    a = x\n",
    "    acts = [a] # list to store all the activations, layer by layer\n",
    "    zs = [] # list to store all the z vectors, layer by layer\n",
    "\n",
    "    for w,b_ in zip(W, b):\n",
    "        z = np.dot(a, w) + b_\n",
    "        zs.append(z)\n",
    "        # apply activation\n",
    "        a = sigmoid(z)\n",
    "        acts.append(a)\n",
    "    y_pred = a\n",
    "    return y_pred, zs, acts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test feed forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction shape: (42000, 10)\n",
      "First prediction: 0\n",
      "Cost with random weights: 12.2411884621\n"
     ]
    }
   ],
   "source": [
    "# test feed forward pass\n",
    "# create network parameters\n",
    "W,b = init_weights([784,100,10])\n",
    "# get predictions\n",
    "pred = feedprop(X_train, W, b)[0]\n",
    "print('Prediction shape:', pred.shape)\n",
    "print('First prediction:', np.argmax(pred[0]))\n",
    "print('Cost with random weights:', categorical_crossentropy(pred, y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We need some accuracy metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def accuracy(x,y,W,b):\n",
    "    '''x - network inputs\n",
    "       y - labels\n",
    "    '''\n",
    "    y_pred = feedprop(x,W,b)[0]\n",
    "    y_pred = np.argmax(y_pred, axis=1)\n",
    "    y = np.argmax(y, axis=1)\n",
    "    acc = np.equal(y_pred, y)\n",
    "    acc = np.mean(acc)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction shape: (42000, 10)\n",
      "First prediction: 9\n",
      "Cost with random weights: 13.6191427977\n",
      "Accuracy: 0.0578571428571\n"
     ]
    }
   ],
   "source": [
    "# test feed forward pass\n",
    "# create network parameters\n",
    "W,b = init_weights([784,100,10])\n",
    "# get predictions\n",
    "pred = feedprop(X_train, W, b)[0]\n",
    "print('Prediction shape:', pred.shape)\n",
    "print('First prediction:', np.argmax(pred[0]))\n",
    "print('Cost with random weights:', categorical_crossentropy(pred, y_train))\n",
    "acc = accuracy(X_train, y_train, W, b)\n",
    "print('Accuracy:', acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def backprop(x, y, W, b):\n",
    "    '''(x,y) - one learning pair\n",
    "       W - weights\n",
    "       b - biases\n",
    "    '''\n",
    "    # variables for storing gradients\n",
    "    grad_b = [np.zeros_like(b_) for b_ in b]\n",
    "    grad_W = [np.zeros_like(w) for w in W]\n",
    "    \n",
    "    # forward pass\n",
    "    # predicted value, neurons values, activations\n",
    "    y_pred, zs, acts = feedprop(x, W, b)\n",
    "    # reshape to matrix\n",
    "    if len(acts[0].shape) == 1:\n",
    "        acts[0] = np.expand_dims(acts[0], axis=1)\n",
    "    # compute error for last layer\n",
    "    e = categorical_crossentropy_derivative(y_pred, y)\n",
    "    #e = e*sigmoid_derivative(zs[-1])\n",
    "    grad_b[-1] = e\n",
    "    grad_W[-1] = np.dot(acts[-2].T, e) # we dont need last activation\n",
    "    # loop back over layers\n",
    "    n_layers = len(b)+1\n",
    "    for l in range(2, n_layers):\n",
    "        z = zs[-l]\n",
    "        # gradient for sigmoid\n",
    "        sigmoid_grad = sigmoid_derivative(z)\n",
    "        e = np.dot(e, W[-l+1].T) * sigmoid_grad\n",
    "        grad_b[-l] = e\n",
    "        grad_W[-l] = np.dot(acts[-l-1], e)\n",
    "        \n",
    "    return grad_W, grad_b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test backpropagation on first 10 training samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 849,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:3: RuntimeWarning: overflow encountered in exp\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "for x,y in zip(X_train[:10],y_train[:10]):\n",
    "    grad_W, grad_b = backprop(x, y, W, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's do Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 868,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def GD(X_train, y_train, W, b, epochs, learning_rate):\n",
    "    for i in range(epochs):\n",
    "        delta_W = [np.zeros_like(w) for w in W]\n",
    "        delta_b = [np.zeros_like(b_) for b_ in b]\n",
    "        \n",
    "        for x,y in zip(X_train, y_train):\n",
    "            grad_W, grad_b = backprop(x,y,W,b)\n",
    "            \n",
    "            for l in range(len(b)):\n",
    "                # compute delta\n",
    "                delta_W[l] += grad_W[l]\n",
    "                delta_b[l] += grad_b[l]\n",
    "                \n",
    "        for l in range(len(b)):\n",
    "            W[l] = W[l] - (learning_rate/len(X_train))*delta_W[l]\n",
    "            b[l] = b[l] - (learning_rate/len(X_train))*delta_b[l]\n",
    "                \n",
    "        # metric\n",
    "        #print([w.shape for w in W])\n",
    "        #print([b_.shape for b_ in b])\n",
    "        \n",
    "        train_acc = accuracy(X_train, y_train, W, b)\n",
    "        print('Epoch {} Accuracy: {}'.format(i, train_acc))\n",
    "    \n",
    "    return W,b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct neural network (init weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 884,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W,b = init_weights([X_train.shape[1],30,N_CLASSES])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do 50 epochs of gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 885,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:3: RuntimeWarning: overflow encountered in exp\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Accuracy: 0.08542857142857142\n",
      "Epoch 1 Accuracy: 0.1345952380952381\n",
      "Epoch 2 Accuracy: 0.21723809523809523\n",
      "Epoch 3 Accuracy: 0.29678571428571426\n",
      "Epoch 4 Accuracy: 0.35604761904761906\n",
      "Epoch 5 Accuracy: 0.3887857142857143\n",
      "Epoch 6 Accuracy: 0.4180238095238095\n",
      "Epoch 7 Accuracy: 0.4504761904761905\n",
      "Epoch 8 Accuracy: 0.4855238095238095\n",
      "Epoch 9 Accuracy: 0.5050714285714286\n",
      "Epoch 10 Accuracy: 0.5241666666666667\n",
      "Epoch 11 Accuracy: 0.5490952380952381\n",
      "Epoch 12 Accuracy: 0.5598095238095238\n",
      "Epoch 13 Accuracy: 0.5618571428571428\n",
      "Epoch 14 Accuracy: 0.5806428571428571\n",
      "Epoch 15 Accuracy: 0.5988809523809524\n",
      "Epoch 16 Accuracy: 0.6019761904761904\n",
      "Epoch 17 Accuracy: 0.6240238095238095\n",
      "Epoch 18 Accuracy: 0.6312619047619048\n",
      "Epoch 19 Accuracy: 0.6386190476190476\n",
      "Epoch 20 Accuracy: 0.6586904761904762\n",
      "Epoch 21 Accuracy: 0.661047619047619\n",
      "Epoch 22 Accuracy: 0.6806666666666666\n",
      "Epoch 23 Accuracy: 0.6765238095238095\n",
      "Epoch 24 Accuracy: 0.6913333333333334\n",
      "Epoch 25 Accuracy: 0.6931904761904762\n",
      "Epoch 26 Accuracy: 0.7070952380952381\n",
      "Epoch 27 Accuracy: 0.7081428571428572\n",
      "Epoch 28 Accuracy: 0.7218095238095238\n",
      "Epoch 29 Accuracy: 0.7269761904761904\n",
      "Epoch 30 Accuracy: 0.7265238095238096\n",
      "Epoch 31 Accuracy: 0.7338809523809524\n",
      "Epoch 32 Accuracy: 0.7413333333333333\n",
      "Epoch 33 Accuracy: 0.746\n",
      "Epoch 34 Accuracy: 0.7458333333333333\n",
      "Epoch 35 Accuracy: 0.7523333333333333\n",
      "Epoch 36 Accuracy: 0.7579285714285714\n",
      "Epoch 37 Accuracy: 0.7605\n",
      "Epoch 38 Accuracy: 0.7654047619047619\n",
      "Epoch 39 Accuracy: 0.769\n",
      "Epoch 40 Accuracy: 0.7711666666666667\n",
      "Epoch 41 Accuracy: 0.7781904761904762\n",
      "Epoch 42 Accuracy: 0.7748571428571429\n",
      "Epoch 43 Accuracy: 0.7819047619047619\n",
      "Epoch 44 Accuracy: 0.7799047619047619\n",
      "Epoch 45 Accuracy: 0.7883571428571429\n",
      "Epoch 46 Accuracy: 0.7858333333333334\n",
      "Epoch 47 Accuracy: 0.7859047619047619\n",
      "Epoch 48 Accuracy: 0.7869761904761905\n",
      "Epoch 49 Accuracy: 0.7888095238095238\n"
     ]
    }
   ],
   "source": [
    "W,b = GD(X_train, y_train, W, b, 50, 0.8)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
